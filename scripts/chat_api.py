import argparse
import os
# [æ–°å¢] è§£å†³ OpenBLAS Warning: Detect OpenMP Loop é—®é¢˜
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"

import subprocess
import tempfile
import re  # å¼•å…¥æ­£åˆ™æ¨¡å—ç”¨äºè§£æ
import urllib.request # ç”¨äºä¸‹è½½éŸ³é¢‘
import time # [æ–°å¢] å¼•å…¥ time æ¨¡å—
import dashscope # å¼•å…¥é˜¿é‡Œäº‘ DashScope SDK
from dashscope.audio.asr import Recognition, RecognitionCallback, RecognitionResult # å¼•å…¥ FunASR ç›¸å…³ç±»

# ç§»é™¤ gpt4allï¼Œå¼•å…¥ openai
from openai import OpenAI
# from playsound import playsound # ç§»é™¤ playsoundï¼Œæ”¹ç”¨ ffplay
import speech_recognition as sr
#from TTS.api import TTS

# --- FunASR ç›¸å…³ç±»å®šä¹‰ ---
class FunASRCallback(RecognitionCallback):
    def __init__(self):
        super().__init__()
        self.final_text = ""
        self.is_finished = False

    def on_open(self) -> None:
        # print('FunASR: Connection open.')
        pass

    def on_close(self) -> None:
        # print('FunASR: Connection close.')
        self.is_finished = True

    def on_complete(self) -> None:
        # print('FunASR: Recognition completed.')
        self.is_finished = True

    def on_error(self, message) -> None:
        print('FunASR Error:', message.message)
        self.is_finished = True

    def on_event(self, result: RecognitionResult) -> None:
        sentence = result.get_sentence()
        if 'text' in sentence:
            # print('FunASR Partial:', sentence['text'])
            if RecognitionResult.is_sentence_end(sentence):
                self.final_text += sentence['text']
                # print('FunASR Sentence End:', sentence['text'])

class FunASRClient:
    def __init__(self):
        if not os.environ.get("DASHSCOPE_API_KEY"):
            print("Warning: DASHSCOPE_API_KEY not found for FunASR.")
        
        # è®¾ç½® API URL (åŒ—äº¬èŠ‚ç‚¹)
        dashscope.base_websocket_api_url = 'wss://dashscope.aliyuncs.com/api-ws/v1/inference'

    def recognize(self, audio_file_path):
        """
        ä½¿ç”¨ FunASR è¯†åˆ«æœ¬åœ°éŸ³é¢‘æ–‡ä»¶
        æ³¨æ„ï¼šè™½ç„¶ FunASR æ˜¯å®æ—¶çš„ï¼Œä½†ä¸ºäº†å…¼å®¹ç°æœ‰çš„â€œå½•éŸ³-è¯†åˆ«â€æµç¨‹ï¼Œ
        æˆ‘ä»¬è¿™é‡Œæ¨¡æ‹Ÿå®æ—¶å‘é€æ•°æ®ã€‚
        """
        callback = FunASRCallback()
        
        # åˆå§‹åŒ–è¯†åˆ«æœåŠ¡
        recognition = Recognition(
            model='fun-asr-realtime',
            format='pcm', 
            sample_rate=16000, # FunASR é€šå¸¸éœ€è¦ 16k
            callback=callback
        )
        
        recognition.start()
        
        # è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶å‘é€
        # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦å°†å½•åˆ¶çš„ 44.1k/åŒå£°é“ éŸ³é¢‘è½¬æ¢ä¸º 16k/å•å£°é“
        # è¿™é‡Œä½¿ç”¨ ffmpeg è½¬æ¢æµ
        cmd = [
            "ffmpeg", 
            "-i", audio_file_path,
            "-f", "s16le",       # è¾“å‡º PCM
            "-ac", "1",          # å•å£°é“
            "-ar", "16000",      # 16000Hz
            "-loglevel", "quiet",
            "pipe:1"             # è¾“å‡ºåˆ° stdout
        ]
        
        try:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
            
            while True:
                data = process.stdout.read(3200) # æ¯æ¬¡è¯»å–ä¸€å°å—
                if not data:
                    break
                recognition.send_audio_frame(data)
                
            process.wait()
            
        except Exception as e:
            print(f"FunASR Audio Processing Error: {e}")
            
        recognition.stop()
        
        # ç­‰å¾…å›è°ƒå®Œæˆï¼ˆç®€å•å¤„ç†ï¼‰
        # import time # [ç§»é™¤] å·²ç»åœ¨é¡¶éƒ¨å¯¼å…¥
        timeout = 5
        start_time = time.time()
        while not callback.is_finished and (time.time() - start_time < timeout):
            time.sleep(0.1)
            
        return callback.final_text

# ------------------------

class QwenTTS:
    def __init__(self, model="qwen3-tts-flash", voice="Dylan"):
        self.model = model
        self.voice = voice
        # ç¡®ä¿ API Key å­˜åœ¨
        if not os.environ.get("DASHSCOPE_API_KEY"):
            print("Warning: DASHSCOPE_API_KEY not found in environment variables.")

    def process(self, text, audio_save_path):
        try:
            # ä½¿ç”¨ MultiModalConversation æ¥å£
            response = dashscope.MultiModalConversation.call(
                model=self.model,
                api_key=os.environ.get("DASHSCOPE_API_KEY"),
                text=text,
                voice=self.voice,
                language_type="Chinese"
            )
            
            if response.status_code == 200:
                # æ£€æŸ¥ response ç»“æ„
                if hasattr(response, 'output') and response.output.audio and response.output.audio.url:
                    audio_url = response.output.audio.url
                    # ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
                    urllib.request.urlretrieve(audio_url, audio_save_path)
                else:
                    print(f"QwenTTS Error: Invalid response structure: {response}")
            else:
                print(f"QwenTTS Error: {response.message}")
                
        except Exception as e:
            print(f"QwenTTS Exception: {e}")


class DoubaoChatBot:
    """Smart Bicycle Voice Assistant based on Whisper/FunASR and Doubao API"""

    def __init__(self, api_key, base_url, model_name, whisper_model_type="base", tts_type="qwen", tts_rate=165, mic_index=None, asr_type="funasr"):
        print(f"==> Doubao Model: {model_name}, ASR: {asr_type}, TTS: {tts_type}")
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (ç”¨äºè±†åŒ… API)
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.model_name = model_name

        self.asr_type = asr_type
        self.whisper_model_type = whisper_model_type

        # åˆå§‹åŒ– ASR å¼•æ“
        if self.asr_type == "whisper":
            self.voice_recognizer = sr.Recognizer()
        elif self.asr_type == "funasr":
            self.funasr_client = FunASRClient()
            self.voice_recognizer = sr.Recognizer() # ä»…ç”¨äºè¯»å–æ–‡ä»¶è¾…åŠ©

        # ä¿å­˜ mic_index ä¾› ffmpeg ä½¿ç”¨
        self.mic_index = mic_index if mic_index is not None else 0

        if tts_type == "qwen":
            self.tts_engine = QwenTTS()

    def run(self):
        """Run the listen-think-response loop"""
        cycle_start = time.time() # [æ–°å¢] è®°å½•å¾ªç¯å¼€å§‹æ—¶é—´

        input_words = self._voice_to_text()
        if input_words:
            # è·å–ç»“æ„åŒ–å›å¤ (æ–‡æœ¬ + åŠ¨ä½œ)
            reply_text, action = self.run_gpt(input_words)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if action:
                self._handle_action(action)
            
            # æ’­æ”¾è¯­éŸ³å›å¤
            self._text_to_voice(reply_text)

            cycle_end = time.time() # [æ–°å¢] è®°å½•å¾ªç¯ç»“æŸæ—¶é—´
            print(f"--- [Timing] Total Interaction Time: {cycle_end - cycle_start:.2f}s ---\n")

    def _voice_to_text(self):
        """Listen voice using ffmpeg and convert to text"""
        print("Listening...")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜å½•éŸ³
        tmp_wav = tempfile.NamedTemporaryFile(suffix=".wav", delete=False).name
        
        # æ„é€ è®¾å¤‡åç§°ï¼Œmain.c ä½¿ç”¨çš„æ˜¯ plughwï¼Œè¿™é‡Œä¿æŒä¸€è‡´
        device_name = f"plughw:{self.mic_index}"
        
        # æ„é€  ffmpeg å‘½ä»¤ (å½•åˆ¶ 5 ç§’)
        cmd = [
            "ffmpeg",
            "-y",
            "-f", "alsa",
            "-i", device_name,
            "-t", "5", 
            "-ar", "44100",
            "-ac", "2",
            "-loglevel", "error", # å‡å°‘æ—¥å¿—è¾“å‡º
            tmp_wav
        ]
        
        try:
            print(f"Recording from {device_name} via ffmpeg (5s)...")
            t_rec_start = time.time() # [æ–°å¢]
            # è°ƒç”¨ç³»ç»Ÿ ffmpeg è¿›è¡Œå½•éŸ³
            subprocess.run(cmd, check=True)
            t_rec_end = time.time() # [æ–°å¢]
            print(f"[Timing] Recording: {t_rec_end - t_rec_start:.2f}s") # [æ–°å¢]
            
            if os.path.exists(tmp_wav) and os.path.getsize(tmp_wav) > 0:
                print("Recognizing...")
                t_asr_start = time.time() # [æ–°å¢]
                transcript = ""
                
                if self.asr_type == "whisper":
                    # ä½¿ç”¨ Whisper
                    with sr.AudioFile(tmp_wav) as source:
                        audio_data = self.voice_recognizer.record(source)
                    transcript = self.voice_recognizer.recognize_whisper(
                        audio_data, self.whisper_model_type
                    )
                elif self.asr_type == "funasr":
                    # ä½¿ç”¨ FunASR
                    transcript = self.funasr_client.recognize(tmp_wav)
                
                t_asr_end = time.time() # [æ–°å¢]
                print(f"[Timing] ASR Recognition: {t_asr_end - t_asr_start:.2f}s") # [æ–°å¢]

                print(f"You said: {transcript}")
                return transcript
            else:
                print("Recording failed: File is empty.")
                return None
                
        except subprocess.CalledProcessError as e:
            print(f"FFmpeg recording error: {e}")
            print("Please check if 'ffmpeg' is installed and the device index is correct.")
            return None
        except Exception as e:
            print(f"Recognition error: {e}")
            return None
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_wav):
                os.remove(tmp_wav)

    def run_gpt(self, question):
        """Run Doubao API model and parse structured output"""
        print("Thinking (Doubao API)...")
        t_llm_start = time.time() # [æ–°å¢]
        
        # å®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå¼ºåˆ¶è§„å®šè¾“å‡ºæ ¼å¼
        system_prompt = """
# Role
You are a smart bicycle assistant. You control the bike and talk to the rider.

# Task Instructions
1. **Bicycle Control**: If the user asks to control the bike (move, turn, stop, follow), output the corresponding action command. DO NOT use web search for these requests.
2. **Knowledge/Weather Query**: If the user asks about general knowledge, weather, news, or location info, use the `web_search` tool to get real-time information.
3. **Language**: Always reply in the same language as the user's input (Chinese or English).

# Output Format
You must output your response in a strict format with two parts:
1. The verbal reply to the user.
2. The action command wrapped in <action> tags.
# Available actions:
- forward (Move forward)
- back (Move backward)
- left (Turn left)
- right (Turn right)
- follow (Follow mode)
- avoid (Obstacle avoidance mode)
- stop (Stop the bike)
- none (No action needed)

Example User: "Go forward please."
Example Output: OK, moving forward now. <action>forward</action>

Example User: "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ" (Use web_search)
Example Output: åŒ—äº¬ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸© 25 åº¦ï¼Œé€‚åˆéª‘è¡Œ

Example User: "Hello."
Example Output: Hello! Ready to ride. <action>none</action>

Keep your verbal reply short and concise.
"""
        tools = [{
            "type": "web_search",
            "max_keyword": 3,
            "sources": ["douyin", "moji"],
            "user_location": {  # ç”¨æˆ·åœ°ç†ä½ç½®ï¼ˆç”¨äºä¼˜åŒ–æœç´¢ç»“æœï¼‰
                "type": "approximate",  # å¤§è‡´ä½ç½®
                "country": "ä¸­å›½",
                "region": "åŒ—äº¬",
                "city": "åŒ—äº¬"
            },
            "limit": 3
        }]
        try:
            # è°ƒç”¨è±†åŒ… API (Response API)
            response = self.client.responses.create(
                model=self.model_name,
                input=[
                    {
                        "role": "system",
                        "content": [{"type": "input_text", "text": system_prompt}]
                    },
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": question}]
                    }
                ],
                tools=tools
            )
            t_llm_end = time.time() # [æ–°å¢]
            print(f"[Timing] LLM Thinking: {t_llm_end - t_llm_start:.2f}s") # [æ–°å¢]

            # print(response)
            
            # ä¿®å¤è§£æé€»è¾‘ï¼šResponse API çš„ç»“æ„ä¸åŒäº ChatCompletion
            raw_answer = ""
            if hasattr(response, 'output'):
                for item in response.output:
                    if item.type == 'message':
                        for content in item.content:
                            if content.type == 'output_text':
                                raw_answer += content.text
            
            print(f"==> Raw LLM Output: {raw_answer}")

            # è§£æè¾“å‡º
            reply_text = raw_answer
            action = "none"

            # ä½¿ç”¨æ­£åˆ™æå– <action>...</action>
            match = re.search(r'<action>(.*?)</action>', raw_answer, re.IGNORECASE)
            if match:
                action = match.group(1).strip().lower()
                # ä»å›å¤æ–‡æœ¬ä¸­ç§»é™¤ action æ ‡ç­¾éƒ¨åˆ†ï¼Œåªä¿ç•™è¯­éŸ³å†…å®¹
                reply_text = raw_answer.replace(match.group(0), "").strip()
            
            print(f"==> Parsed Reply: {reply_text}")
            print(f"==> Parsed Action: {action}")
            
            return reply_text, action

        except Exception as e:
            print(f"API Error: {e}")
            return "Sorry, I encountered an error.", "none"

    def _handle_action(self, action):
        """Execute the bicycle control action"""
        if action == "none":
            return
            
        print(f"\n>>> ğŸš² BICYCLE ACTION EXECUTED: [{action.upper()}] <<<\n")
        # åœ¨è¿™é‡Œæ·»åŠ å®é™…çš„ç¡¬ä»¶æ§åˆ¶ä»£ç 
        # ä¾‹å¦‚: serial.write(action.encode())

    def _text_to_voice(self, answer):
        """Convert text to voice using TTS tools"""
        if not answer:
            return

        tmp_file = tempfile.NamedTemporaryFile(
            prefix="talkgpt4all-", suffix=".wav", delete=False
        )
        # ç«‹å³å…³é—­æ–‡ä»¶å¥æŸ„
        tmp_file.close()
        
        try:
            t_tts_gen_start = time.time() # [æ–°å¢]
            try:
                self.tts_engine.process(answer, tmp_file.name)
            except RuntimeError as e:
                print(f"TTS Error: {e}")
                return
            t_tts_gen_end = time.time() # [æ–°å¢]
            print(f"[Timing] TTS Generation: {t_tts_gen_end - t_tts_gen_start:.2f}s") # [æ–°å¢]

            # [ä¿®å¤ 2] æ›¿æ¢ playsoundï¼Œç›´æ¥è°ƒç”¨ç³»ç»ŸéªŒè¯è¿‡çš„ ffplay
            # -autoexit: æ’­æ”¾å®Œè‡ªåŠ¨é€€å‡º
            # -nodisp: ä¸æ˜¾ç¤ºå›¾å½¢çª—å£
            # -ss 0: ä»å¤´æ’­æ”¾
            print(f"Playing audio using ffplay: {tmp_file.name}")
            t_play_start = time.time() # [æ–°å¢]
            subprocess.run(
                ["ffplay", "-autoexit", "-nodisp", "-hide_banner", "-loglevel", "error", tmp_file.name], 
                check=False
            )
            t_play_end = time.time() # [æ–°å¢]
            print(f"[Timing] Audio Playback: {t_play_end - t_play_start:.2f}s") # [æ–°å¢]
            
        finally:
            if os.path.exists(tmp_file.name):
                os.remove(tmp_file.name)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    
    # è±†åŒ… API ç›¸å…³å‚æ•°
    parser.add_argument(
        "--api-key",
        type=str,
        default=os.environ.get("ARK_API_KEY"),
        help="Doubao API Key (or set ARK_API_KEY env var)",
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default="https://ark.cn-beijing.volces.com/api/v3",
        help="Doubao Base URL",
    )
    parser.add_argument(
        "-m",
        "--model-name",
        type=str,
        default="doubao-seed-1-6-flash-250828", # ä½ çš„æ¥å…¥ç‚¹ ID
        help="Doubao Endpoint ID / Model Name",
    )
    
    # ASR ç›¸å…³å‚æ•°
    parser.add_argument(
        "--asr-type",
        type=str,
        default="whisper",
        choices=["whisper", "funasr"],
        help="ASR engine type: whisper (local) or funasr (cloud)",
    )
    parser.add_argument(
        "-w",
        "--whisper-model-type",
        type=str,
        default="base",
        help="whisper model type, default is base",
    )
    
    # TTS ç›¸å…³å‚æ•°
    parser.add_argument(
        "--tts-type",
        type=str,
        default="qwen",
        choices=["qwen"],
        help="TTS engine type: glow (local) or qwen (cloud)",
    )
    parser.add_argument(
        "--voice-rate",
        type=int,
        default=165,
        help="voice rate, default is 165, the larger the speak faster",
    )
    
    # 1. æ·»åŠ éº¦å…‹é£ç´¢å¼•å‚æ•°
    parser.add_argument(
        "--mic-index",
        type=int,
        default=0, # é»˜è®¤ä¸º None (ä½¿ç”¨ç³»ç»Ÿé»˜è®¤)
        help="Microphone device index (use search_device.py to find)",
    )
    
    args = parser.parse_args()

    if not args.api_key:
        print("Error: API Key is required. Please provide --api-key or set ARK_API_KEY environment variable.")
        exit(1)

    chat_bot = DoubaoChatBot(
        api_key=args.api_key,
        base_url=args.base_url,
        model_name=args.model_name,
        whisper_model_type=args.whisper_model_type,
        tts_type=args.tts_type,
        tts_rate=args.voice_rate,
        mic_index=args.mic_index,
        asr_type=args.asr_type # ä¼ å…¥ ASR ç±»å‹
    )
    
    print("System ready. Please speak.")
    while True:
        chat_bot.run()
